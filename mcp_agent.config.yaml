$schema: ./schema/mcp-agent.config.schema.json
anthropic: null
# Search server options: serper, brave, bocha-mcp
default_search_server: serper
document_segmentation:
  enabled: false
  size_threshold_chars: 50000
execution_engine: asyncio
logger:
  level: info
  path_settings:
    path_pattern: logs/mcp-agent-{unique_id}.jsonl
    timestamp_format: '%Y%m%d_%H%M%S'
    unique_id: timestamp
  progress_display: false
  transports:
  - console
  - file
mcp:
  servers:
    bocha-mcp:
      args:
      - tools/bocha_search_server.py
      command: python3
      env:
        BOCHA_API_KEY: ''
        PYTHONPATH: .
    serper:
      args:
      - tools/serper_search_server.py
      command: python
      description: Serper Google Search MCP server - provides web, news, and image search via Google
      env:
        SERPER_API_KEY: ''
        PYTHONPATH: .
    brave:
      # macos and linux should use this
      # args:
      # - -y
      # - '@modelcontextprotocol/server-brave-search'
      # command: npx

      # windows should use this
      args:
      # please use the correct path for your system
      - C:/Users/LEGION/AppData/Roaming/npm/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js
      command: node
      env:
        BRAVE_API_KEY: 'YOUR_BRAVE_API_KEY_HERE'
    filesystem:
      # macos and linux should use this
      args:
      - -y
      - '@modelcontextprotocol/server-filesystem'
      - .
      command: npx

      # windows should use this
      # args:
      # # please use the correct path for your system
      # - C:/Users/LEGION/AppData/Roaming/npm/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js
      # - .
      # command: node


    code-implementation:
      args:
      - tools/code_implementation_server.py
      command: python
      description: Paper code reproduction tool server - provides file operations,
        code execution, search and other functions
      env:
        PYTHONPATH: .
    code-reference-indexer:
      args:
      - tools/code_reference_indexer.py
      command: python
      description: Code reference indexer server - Provides intelligent code reference
        search from indexed repositories
      env:
        PYTHONPATH: .
    command-executor:
      args:
      - tools/command_executor.py
      command: python
      env:
        PYTHONPATH: .
    document-segmentation:
      args:
      - tools/document_segmentation_server.py
      command: python
      description: Document segmentation server - Provides intelligent document analysis
        and segmented reading to optimize token usage
      env:
        PYTHONPATH: .
    fetch:
      args:
      - mcp-server-fetch
      command: uvx
    file-downloader:
      args:
      - tools/pdf_downloader.py
      command: python
      env:
        PYTHONPATH: .
    github-downloader:
      args:
      - tools/git_command.py
      command: python
      env:
        PYTHONPATH: .
# LLM Provider Selection Priority (选择使用哪个LLM / Choose which LLM to use)
# ================================================================================
# HOW PROVIDER SELECTION WORKS:
# 1. If 'llm_provider' is set below, the system tries that provider FIRST
# 2. If the preferred provider is unavailable (no API key or connection fails),
#    the system automatically tries other available providers in order:
#    anthropic -> google -> openai -> openrouter -> ollama
# 3. This ensures the application always works if ANY provider is configured
#
# OPTIONS: "anthropic", "google", "openai", "openrouter", "ollama"
# EXAMPLE: Set llm_provider: "ollama" to prefer local Ollama models
# ================================================================================
llm_provider: "openrouter"  # Set to your preferred provider

openai:
  base_max_tokens: 40000
  # default_model: google/gemini-2.5-pro
  default_model: google/gemini-2.0-flash-exp:free
  # default_model: openai/gpt-oss-120b
  # default_model: deepseek/deepseek-v3.2-exp
  # default_model: moonshotai/kimi-k2-thinking
  reasoning_effort: low  # Only for thinking models
  max_tokens_policy: adaptive
  retry_max_tokens: 32768
  base_url: "https://openrouter.ai/api/v1"  # Use OpenRouter as the default endpoint
  api_key: ""  # Set via environment variable or mcp_agent.secrets.yaml

# Configuration for Google AI (Gemini)
google:
  default_model: "gemini-3-pro-preview"

anthropic:
  default_model: "claude-sonnet-4.5"

# Configuration for OpenRouter (supports multiple models)
openrouter:
  default_model: "google/gemini-2.0-flash-exp:free"  # Free model alternative
  base_max_tokens: 40000
  retry_max_tokens: 32768

# Configuration for Ollama (local models)
# ⚠️  IMPORTANT: Your Ollama model MUST support tool/function calling!
# ⚠️  Compatible models: llama3.1, llama3.2, qwen2.5, mistral, etc.
# ⚠️  NOT compatible: older models without tool support
# Test your model: ollama run <model> "Can you use tools?"
ollama:
  default_model: "qwen2.5-coder:7b-instruct-q5_k_m"  # Change to your installed Ollama model with tool support
  base_max_tokens: 4096
  retry_max_tokens: 2048

planning_mode: traditional
